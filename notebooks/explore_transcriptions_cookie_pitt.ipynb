{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import glob2\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "data_path = '../data/interim'\n",
    "\n",
    "dementia_pickles = [ Path(p).absolute() for p in glob2.glob(data_path + '/dementia/pitt/cookie/*') ]\n",
    "control_pickles = [ Path(p).absolute() for p in glob2.glob(data_path + '/control/pitt/cookie/*') ]\n",
    "\n",
    "def unpickle_all_files(aList):\n",
    "    result = {}\n",
    "    for aPath in aList:\n",
    "        with open(str(aPath), 'rb') as handle:\n",
    "            b = pickle.load(handle)\n",
    "        result[aPath.stem] = b\n",
    "    return result\n",
    "\n",
    "dementia_data = unpickle_all_files(dementia_pickles)\n",
    "control_data = unpickle_all_files(control_pickles)\n",
    "\n",
    "dementia_texts = {k:v['text'] for k,v in dementia_data.items()}\n",
    "control_texts = {k:v['text'] for k,v in control_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Tell me everything you see happening there, everything that's going on in that picture. Okay, well the little boy is stealing some cookies, taking some cookies when his mother's not looking and he's going to fall off the stool because it's tipping over and the little girl is saying, she reaches up for a cookie and pretty soon it's not going to be very quiet. The mother is wiping the dishes as the sink is overflowing on the floor and she's walking in the water and apparently unconcerned. And the window is open. As far as things happening, I guess that's about it. She's wiping the dishes.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dementia_texts['120-4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of DEMENTIA transcription:\n",
      " I want you to take a look at that picture. Oh boy. I want you to tell me everything you see happening there. Everything that's going on. The kids are trying to get the cookies. And she's laughing but holding her hand up to them. And they know he's going to fall off the chair. And their mother is doing the dishes. And not being careful she didn't turn the water off. And the water is all going to the floor. Instead of getting the dishes all dry. That's all. I can tuck two cups in a plate. That's it. Thanks.\n",
      "\n",
      "Example of CONTROL transcription:\n",
      " I'm going to show you a picture. And in the picture, there's some things going on. I want you to do is just look at the picture and describe to me any action you see taking place or anything you see happening. The kids are swiping some cookies there, and the stool is upsetting. And the kid, the girl's telling her to be quiet. So his mother over there washing dishes, and the sink's overflowing. And the window's up. And I don't see much more than that. She's drying dishes. Did you say action? Right. I have an idea. Cookie jar lid's falling.\n"
     ]
    }
   ],
   "source": [
    "# see examples\n",
    "print('Example of DEMENTIA transcription:')\n",
    "dementia_sample = list(dementia_texts.values())[np.random.randint(len(dementia_texts.values()))]\n",
    "print(dementia_sample)\n",
    "print('\\nExample of CONTROL transcription:')\n",
    "control_sample = list(control_texts.values())[np.random.randint(len(control_texts.values()))]\n",
    "print(control_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "final_transcript = '[CLS]' + ' [SEP]'.join([a['text'] for a in dementia_data['078-0']['segments']]) + ' [SEP]'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(final_transcript, return_tensors=\"pt\", truncation=True, max_length=500, padding='max_length')\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0844,  0.0845,  0.1268,  ..., -0.0295,  0.3428,  0.3118],\n",
       "        [-0.0476,  0.1092,  0.1339,  ..., -0.0294,  0.3267,  0.3059],\n",
       "        [ 0.4353, -0.0547,  0.3989,  ...,  0.4625,  0.7648,  0.7710],\n",
       "        ...,\n",
       "        [ 0.2075,  0.0735,  0.0068,  ...,  0.1290,  0.0050, -0.2418],\n",
       "        [ 0.0626, -0.0365, -0.0858,  ...,  0.2477, -0.0697, -0.0681],\n",
       "        [ 0.0350,  0.0222, -0.0436,  ...,  0.2180, -0.0666,  0.0080]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 500, 768])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.08441833,  0.08449329,  0.12680854, ..., -0.02945847,\n",
       "          0.34278136,  0.3118286 ],\n",
       "        [-0.04758149,  0.10920547,  0.13391875, ..., -0.029373  ,\n",
       "          0.3267451 ,  0.3059093 ],\n",
       "        [ 0.43527514, -0.05470229,  0.39889002, ...,  0.46251014,\n",
       "          0.7648194 ,  0.7710148 ],\n",
       "        ...,\n",
       "        [ 0.20745543,  0.07352588,  0.00680953, ...,  0.1289518 ,\n",
       "          0.0049639 , -0.24180306],\n",
       "        [ 0.06263701, -0.03647479, -0.08579326, ...,  0.24769208,\n",
       "         -0.06973306, -0.06805833],\n",
       "        [ 0.03499333,  0.02217047, -0.04363602, ...,  0.21797748,\n",
       "         -0.06661072,  0.00801057]]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio, valid_ratio, test_ratio = 0.6, 0.2, 0.2\n",
    "X = list(control_texts.values())+list(dementia_texts.values())\n",
    "y = [0]*len(control_texts.values())+[1]*len(dementia_texts.values())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_ratio, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with BERT & Pytorch\n",
    "Reference: https://www.kaggle.com/code/joydeb28/text-classification-with-bert-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e588c3c6a6b54880ab7836fb1b17f434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_len = [len(i.split()) for i in X_train]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)\n",
    "max_seq_len = max(seq_len)\n",
    "print(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/c1/d/164546/miniconda3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "if max_seq_len>512:\n",
    "    max_seq_len = 512\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    X_train,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    X_valid,\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y: tensor([0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0])\n",
      "val_y: tensor([0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(y_train)\n",
    "print(\"train_y:\",train_y)\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(y_valid)\n",
    "print(\"val_y:\",val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 16\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert,output_size):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,output_size)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert,2)\n",
    "\n",
    "# push the model to GPU\n",
    "device='cpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zhome/c1/d/164546/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.11538462 0.90625   ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "weights = weights.to(device)\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    total_labels =[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        # append the model predictions\n",
    "        total_preds+=list(preds)\n",
    "        total_labels+=labels.tolist()\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    #total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    f1 = f1_score(total_labels, total_preds, average='weighted')\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b436ce4aa9a050b3c12f13399e5d5f02b520991d83a91cd5126888583b80ec2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
